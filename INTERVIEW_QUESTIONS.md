# 项目面试题（基于给定仓库）

## 一、整体设计与架构

1. [进阶] MiniMind 选择从零实现完整 LLM 训练链路（预训练、SFT、LoRA、DPO、PPO/GRPO/SPO 等）的动机是什么？与直接使用 Transformers/TRL 高层封装相比，在可控性与开发成本上有哪些取舍？
2. [进阶] 项目中 `model_minimind` 与各训练脚本（如 `train_pretrain.py`、`train_dpo.py`、`train_ppo.py` 等）的模块职责边界如何划分？在扩展新算法时如何避免耦合和重复代码？
3. [基础] 当前训练入口普遍依赖命令行参数配置（batch、lr、hidden_size、max_seq_len 等）。如果需要支持统一的多阶段流水线（预训练→SFT→RL），你会如何设计配置管理与参数传递？
4. [进阶] 项目提供 Dense 与 MoE 两种架构。请说明在训练管线中 MoE 模式下需要额外关注的状态或检查点处理逻辑，以及与 Dense 共享的部分是如何复用的。
5. [高阶] 若要把 MiniMind 扩展为分布式多机多卡训练，现有的 DDP 初始化和检查点恢复逻辑是否足够？需要在数据切分、梯度聚合、故障恢复方面做哪些改造？
6. [进阶] 训练脚本采用混合精度与梯度累积来适配消费级 GPU。请分析在高吞吐场景下的潜在瓶颈（数据加载、通信、显存碎片等），并给出优化建议。
7. [基础] 模型在推理时支持 KV cache、RoPE 外推（YaRN）和可选 Flash Attention。整体推理链路如何组织？哪些特性依赖于训练阶段的配置保持一致？
8. [高阶] 如果要将 MiniMind 做成可插拔的研究平台（新增自定义层、采样策略、奖励函数等），你会如何重构当前文件结构与抽象接口，以便最小化对现有脚本的修改？

## 二、关键模块与代码实现

### 模块 A：模型结构（`model/model_minimind.py`）

1. [基础] MiniMind 使用 RMSNorm、RoPE 和多头注意力的实现细节是什么？请解释 `apply_rotary_pos_emb` 的张量形状变化及其在 KV cache 下的处理方式。
2. [进阶] `MiniMindConfig` 支持 `num_key_value_heads` 与 `num_attention_heads` 不同。请说明 `repeat_kv` 的作用以及在推理阶段对内存与速度的影响。
3. [进阶] MoE 路由器如何计算专家选择概率？`aux_loss_alpha`、`seq_aux`、`norm_topk_prob` 等配置项对负载均衡和梯度稳定性的影响是什么？
4. [基础] 模型在 forward 中如何处理 `past_key_value` 缓存？在开启 `use_cache` 时，返回的缓存结构如何被下游推理循环复用？
5. [高阶] 如果要在 MiniMind 中加入分组查询注意力（Grouped Query Attention）或滑动窗口注意力，应该修改哪些层/缓存逻辑以保持兼容现有的生成与训练脚本？

### 模块 B：预训练与数据流（`trainer/train_pretrain.py` 与 `dataset/lm_dataset.py`）

1. [基础] `PretrainDataset` 如何构建输入与标签（X/Y）以及 `loss_mask`？在截断/填充策略上如何避免跨样本的拼接造成语义污染？
2. [进阶] `train_pretrain.py` 中的学习率通过 `get_lr` 余弦退火实现。请解释 warmup 逻辑和步数计算方式，若要增加线性 warmup + 余弦退火需要改哪些地方？
3. [进阶] 混合精度与 `GradScaler` 在训练循环中的调用顺序是什么？为什么在累积步数上需要手动除以 `accumulation_steps`？
4. [基础] 检查点保存使用 `lm_checkpoint`，其中会存储模型、优化器、scaler 与步数。请说明在修改 batch size 或 world size 续训时，步数调整和学习率调度是否仍然一致，如何验证？
5. [高阶] 目前数据加载使用 `DistributedSampler` + `DataLoader`。在极大数据集或远程存储场景下，如何通过异步预取、分片缓存或流式数据管线降低 I/O 对训练的影响？

### 模块 C：对齐与强化学习训练（`trainer/train_dpo.py`, `train_ppo.py`, `train_grpo.py`, `train_spo.py`）

1. [基础] DPO 训练如何构造正负样本对？在 `loss` 计算时如何利用 `beta` 超参控制模型输出与参考策略的距离？
2. [进阶] PPO 训练脚本中 Actor/Critic 的梯度更新顺序、优势函数（GAE）计算以及 `clip` 比例的实现细节是什么？为什么需要额外的 KL 正则或奖励缩放？
3. [进阶] GRPO 通过组内标准化优势替代 Critic。请解释代码中如何生成多条回答、计算组均值/方差，并在退化组（奖励几乎相等）时如何防止学习信号消失。
4. [基础] SPO 采用单流自适应 baseline。它在实现上如何维护跨 batch 的值追踪器？相比 GRPO，它的 KL 约束与归一化步骤有哪些差异？
5. [高阶] 在 RL 阶段，分布式/并行采样与训练交替进行时如何保证参数一致性与经验新鲜度？若要支持异步采样器或经验回放缓存，需要在现有脚本中添加哪些同步或版本控制机制？

## 三、领域知识与工程实践

1. [基础] 解释 RoPE 外推（如 YaRN 参数 `factor`、`beta_fast`、`beta_slow`）对长序列建模的作用和潜在风险（如位置漂移）。
2. [基础] 在小模型上训练 MoE 时，如何平衡专家数量与路由开销？如果专家过多或路由不均衡会导致哪些性能或收敛问题？
3. [进阶] 针对小参数量模型的优化，为什么选择较小词表（如 6.4k）并限制最大序列长度？这对 perplexity、推理速度和部署内存有什么影响？
4. [进阶] 在偏好对齐或 RL 阶段，如何设计奖励函数/过滤规则以避免模型学到投机策略（reward hacking）？
5. [高阶] 若要在消费级 GPU 上实现高吞吐训练，如何权衡 Flash Attention、张量并行、梯度检查点、ZeRO 等技术的组合？各自对带宽、显存和实现复杂度的影响是什么？

## 四、开放式与高阶思考

1. [高阶] 如果要把 MiniMind 做成可在线更新的教学演示平台（浏览器推理 + 可视化训练曲线），你会如何拆分前后端与推理服务？模型权重更新如何做到热加载且保证一致性？
2. [高阶] 当模型需要支持多语言或多模态扩展时，数据集、分词器、位置编码和输出头需要做哪些调整？如何在保持轻量化的同时避免灾难性遗忘？
3. [高阶] 假设要把 MiniMind 迁移到移动端（MNN/NCNN），为了兼顾速度与精度，你会如何裁剪或替换现有算子（例如 KV cache 管理、量化策略、RoPE 计算）？
4. [高阶] 若要实现多租户的 RLHF 服务（不同租户自定义奖励/偏好），在训练与推理的隔离、资源配额和模型版本管理上有哪些挑战？如何基于当前仓库演进？

